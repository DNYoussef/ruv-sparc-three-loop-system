# Neural Network Architecture Configuration Template
#
# This template defines a complete neural network architecture that can be
# used with model-architect.py to generate PyTorch model code.

# Model metadata
name: CustomNeuralNetwork
description: Example neural network architecture
version: 1.0.0
author: ML Expert Skill
created: 2025-11-02

# Input/output specifications
input_shape: [784]  # MNIST-style flattened 28x28 image
output_shape: [10]  # 10-class classification

# Target constraints (optional)
target_params: 25000000  # 25M parameters target
constraints:
  max_memory_mb: 6000  # 6GB VRAM limit
  max_inference_ms: 100  # <100ms inference time
  param_tolerance: 0.1  # Â±10% parameter count tolerance

# Layer definitions
layers:
  # Input projection
  - type: linear
    input_dim: 784
    output_dim: 512
    activation: relu
    dropout: 0.2
    batch_norm: true
    params: {}

  # Hidden layers
  - type: linear
    input_dim: 512
    output_dim: 256
    activation: relu
    dropout: 0.2
    batch_norm: true
    params: {}

  - type: linear
    input_dim: 256
    output_dim: 128
    activation: relu
    dropout: 0.2
    batch_norm: true
    params: {}

  # Output layer
  - type: linear
    input_dim: 128
    output_dim: 10
    activation: softmax
    dropout: 0.0
    batch_norm: false
    params:
      dim: 1

# Advanced architecture patterns (optional)
patterns:
  # Residual connections
  residual:
    enabled: false
    skip_layers: 2  # Skip every 2 layers

  # Attention mechanisms
  attention:
    enabled: false
    type: self_attention
    num_heads: 8

  # Recurrent layers
  recurrent:
    enabled: false
    type: lstm
    hidden_size: 256
    num_layers: 2

# Training configuration (optional, for reference)
training:
  optimizer: adam
  learning_rate: 0.001
  batch_size: 32
  epochs: 100
  loss_function: cross_entropy

  # Learning rate scheduling
  scheduler:
    type: reduce_on_plateau
    factor: 0.5
    patience: 10

  # Regularization
  regularization:
    l2_lambda: 0.0001
    gradient_clipping: 1.0

# Model variants (optional)
variants:
  small:
    description: Reduced parameter count for faster training
    layers_override:
      - {input_dim: 784, output_dim: 256}
      - {input_dim: 256, output_dim: 128}
      - {input_dim: 128, output_dim: 10}

  large:
    description: Increased capacity for better performance
    layers_override:
      - {input_dim: 784, output_dim: 1024}
      - {input_dim: 1024, output_dim: 512}
      - {input_dim: 512, output_dim: 256}
      - {input_dim: 256, output_dim: 10}

# Example usage:
# python model-architect.py --config architecture-template.yaml --output model.py
