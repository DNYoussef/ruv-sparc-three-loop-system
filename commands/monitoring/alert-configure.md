---
name: alert-configure
category: monitoring
version: 1.0.0
---

# /alert-configure

Configure alerting rules, thresholds, and notification channels for proactive monitoring.

## Usage
```bash
/alert-configure [alert_type] [options]
```

## Parameters
- `alert_type` - Alert category: performance|errors|infrastructure|custom (default: all)
- `--severity` - Alert severity: critical|high|warning|info (default: all)
- `--threshold-error-rate` - Error rate threshold % (default: 5)
- `--threshold-latency` - Latency threshold in ms (default: 1000)
- `--threshold-cpu` - CPU usage threshold % (default: 80)
- `--threshold-memory` - Memory usage threshold % (default: 85)
- `--channels` - Notification channels (comma-separated, default: slack)
- `--environment` - Target environment (default: current)

## What It Does

**Comprehensive Alerting Setup**:
1. ðŸš¨ **Alert Rules**: Define conditions and thresholds
2. ðŸ“¢ **Notification Routing**: Channel selection per severity
3. ðŸ”” **Escalation Policies**: Progressive escalation
4. ðŸŽ¯ **Threshold Tuning**: Environment-specific thresholds
5. ðŸ“Š **Alert Grouping**: Reduce noise with intelligent grouping
6. â° **Schedule-Based**: Time windows and maintenance modes
7. ðŸ”• **Silencing**: Manual and auto-silence capabilities
8. ðŸ“ˆ **Alert Analytics**: Track alert trends

## Examples

```bash
# Configure all alerts
/alert-configure

# Performance alerts only
/alert-configure performance --threshold-latency 500

# Critical alerts with custom thresholds
/alert-configure --severity critical --threshold-error-rate 1

# Multi-channel notification
/alert-configure --channels slack,pagerduty,email

# Infrastructure alerts
/alert-configure infrastructure --threshold-cpu 70 --threshold-memory 80

# Custom alert rules
/alert-configure custom --environment production
```

## Output

```
ðŸš¨ Alert Configuration Started

Environment: production
Alert Types: all
Severity Levels: all
Channels: slack, pagerduty, email

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Critical Alerts (Immediate Response)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  âœ… ServiceDown
     Condition: ALL instances unavailable for 1m
     Threshold: 0 healthy instances
     Channels: PagerDuty, Slack (#incidents), Phone
     Escalation: Immediate â†’ On-call engineer
     Auto-page: Yes

  âœ… CriticalErrorRate
     Condition: Error rate >10% for 5m
     Threshold: 10% errors
     Channels: PagerDuty, Slack (#incidents)
     Escalation: 5m â†’ Engineering lead
     Current: 0.4% (OK)

  âœ… DatabaseDown
     Condition: Primary database unreachable for 30s
     Threshold: Connection timeout
     Channels: PagerDuty, Slack (#database), Phone
     Escalation: Immediate â†’ DBA team
     Auto-failover: Enabled

  âœ… OutOfMemory
     Condition: Container OOM killed
     Threshold: Immediate on OOM event
     Channels: PagerDuty, Slack (#infrastructure)
     Escalation: Immediate â†’ Platform team
     Auto-restart: Enabled

  âœ… DiskFull
     Condition: Disk usage >95% for 2m
     Threshold: 95% usage
     Channels: PagerDuty, Slack (#infrastructure)
     Escalation: 10m â†’ Platform team
     Auto-cleanup: Enabled (logs, temp files)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
High Priority Alerts (Fast Response)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  âœ… HighLatency
     Condition: p95 latency >1s for 10m
     Threshold: 1000ms (p95)
     Channels: Slack (#performance)
     Escalation: 15m â†’ Performance team
     Current: 287ms (OK)

  âœ… HighErrorRate
     Condition: Error rate >5% for 10m
     Threshold: 5% errors
     Channels: Slack (#alerts)
     Escalation: 15m â†’ On-call
     Current: 0.4% (OK)

  âœ… HighCPU
     Condition: CPU >80% for 15m
     Threshold: 80% usage
     Channels: Slack (#infrastructure), Email
     Escalation: 30m â†’ Platform team
     Auto-scale: Triggered at 85%
     Current: 45% (OK)

  âœ… HighMemory
     Condition: Memory >85% for 15m
     Threshold: 85% usage
     Channels: Slack (#infrastructure), Email
     Escalation: 30m â†’ Platform team
     Auto-scale: Triggered at 90%
     Current: 62% (OK)

  âœ… SlowDatabaseQueries
     Condition: Query duration >5s for 5m
     Threshold: 5000ms
     Channels: Slack (#database)
     Escalation: 20m â†’ DBA team
     Auto-log: Slow query logging enabled

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Warning Alerts (Monitor & Plan)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  âœ… ElevatedErrorRate
     Condition: Error rate >1% for 30m
     Threshold: 1% errors
     Channels: Email
     Escalation: None (informational)
     Current: 0.4% (OK)

  âœ… DiskSpaceLow
     Condition: Disk usage >85% for 1h
     Threshold: 85% usage
     Channels: Email (daily digest)
     Escalation: None
     Current: 67% (OK)

  âœ… CertificateExpiry
     Condition: SSL cert expires in <30 days
     Threshold: 30 days
     Channels: Email (weekly)
     Escalation: None
     Renewal: Auto-renewal enabled (Let's Encrypt)

  âœ… LowCacheHitRatio
     Condition: Cache hit ratio <70% for 2h
     Threshold: 70% hit ratio
     Channels: Slack (#performance)
     Escalation: None
     Current: 76% (OK)

  âœ… PodRestartLoop
     Condition: Pod restarted >3 times in 1h
     Threshold: 3 restarts
     Channels: Slack (#infrastructure)
     Escalation: None
     Auto-debug: Logs collected

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Notification Channel Configuration
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  ðŸ“± Slack Integration
     Workspace: mycompany.slack.com
     Channels configured:
       âœ… #incidents (critical alerts)
       âœ… #alerts (high priority)
       âœ… #performance (latency, throughput)
       âœ… #infrastructure (CPU, memory, disk)
       âœ… #database (DB-related)
     Webhook: https://hooks.slack.com/services/T...
     Format: Rich formatting with graphs
     Mentions: @oncall for critical

  ðŸ“ž PagerDuty Integration
     Service: Production Alerts
     Integration key: â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢
     Escalation policy:
       L1: On-call engineer (immediate)
       L2: Engineering lead (+5m)
       L3: CTO (+15m)
     Auto-resolve: Yes
     Acknowledge timeout: 15m

  ðŸ“§ Email Integration
     SMTP: smtp.sendgrid.com:587
     From: alerts@example.com
     Recipients:
       Critical: ops-team@example.com, oncall@example.com
       High: engineering@example.com
       Warning: devops@example.com (daily digest)
     HTML formatting: Enabled
     Inline graphs: Yes

  ðŸ“² Webhook Integration
     Endpoints:
       âœ… https://api.example.com/webhooks/alerts
       âœ… https://ops-dashboard.example.com/alerts
     Format: JSON
     Authentication: Bearer token
     Retry: 3 attempts with backoff

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Alert Routing & Grouping
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  ðŸ”€ Routing Rules:
     âœ… severity=critical â†’ PagerDuty + Slack
     âœ… severity=high â†’ Slack + Email
     âœ… severity=warning â†’ Email (digest)
     âœ… alertname=DatabaseDown â†’ DBA team
     âœ… namespace=production â†’ Immediate escalation

  ðŸ“¦ Grouping Configuration:
     âœ… Group by: alertname, cluster, namespace
     âœ… Group wait: 30s (collect similar alerts)
     âœ… Group interval: 5m (send grouped alerts)
     âœ… Repeat interval: 4h (re-notify if unresolved)

  ðŸ”• Inhibition Rules:
     âœ… Critical alerts inhibit warnings
     âœ… ServiceDown inhibits HighLatency
     âœ… DatabaseDown inhibits SlowQueries
     âœ… Reduce noise by 60-70%

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Schedule & Maintenance Windows
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  â° Business Hours Routing:
     Weekdays 9am-5pm EST:
       âœ… Critical â†’ PagerDuty + Slack
       âœ… High â†’ Slack
       âœ… Warning â†’ Email

     After hours & weekends:
       âœ… Critical â†’ PagerDuty (immediate)
       âœ… High â†’ PagerDuty (+15m escalation)
       âœ… Warning â†’ Email (next business day)

  ðŸ”§ Maintenance Windows:
     âœ… Scheduled maintenance: Auto-silence alerts
     âœ… Deployment windows: Suppress deployment-related alerts
     âœ… Backup windows: Suppress DB alerts during backups
     Example: Every Sunday 2am-4am EST

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Alert Testing & Validation
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  ðŸ§ª Sending test alerts...

    âœ… Test alert: HighErrorRate (warning)
       Slack: âœ… Received in #alerts (3.2s)
       Email: âœ… Delivered to engineering@example.com (4.5s)

    âœ… Test alert: ServiceDown (critical)
       PagerDuty: âœ… Incident created (2.1s)
       Slack: âœ… Posted to #incidents with @oncall (2.8s)
       Phone: âœ… Call initiated to on-call (5.4s)

    âœ… Test alert: HighCPU (high)
       Slack: âœ… Received in #infrastructure (3.1s)
       Email: âœ… Delivered to devops@example.com (4.2s)

  âœ… All notification channels working
  âœ… Routing rules validated
  âœ… Escalation policies tested

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Alert Configuration Summary
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Status: âœ… COMPLETE
Environment: production
Total Duration: 2m 34s

Alert Rules Created:
  Critical: 5 rules
  High: 5 rules
  Warning: 5 rules
  Total: 15 rules

Notification Channels:
  âœ… Slack (4 channels)
  âœ… PagerDuty (1 service)
  âœ… Email (3 distribution lists)
  âœ… Webhooks (2 endpoints)

Routing Configuration:
  âœ… Severity-based routing
  âœ… Alert grouping (30s wait, 5m interval)
  âœ… Inhibition rules (3 rules)
  âœ… Business hours scheduling

Current Alert Status:
  Active alerts: 0
  Silenced alerts: 0
  Firing rate (24h): 12 alerts
  Resolution rate (24h): 100%
  False positive rate: 2.1%

Thresholds Configured:
  Error rate: >5% (high), >10% (critical)
  Latency: >1000ms p95 (high)
  CPU: >80% (high)
  Memory: >85% (high)
  Disk: >85% (warning), >95% (critical)

Access:
  Alert Manager UI: http://alertmanager.prod.svc:9093
  Grafana Alerts: http://grafana.prod.svc:3000/alerting
  Slack workspace: mycompany.slack.com
  PagerDuty: https://mycompany.pagerduty.com

Next Steps:
  1. Monitor alert firing rates and adjust thresholds
  2. Review escalation policies after first week
  3. Tune alert grouping to reduce noise
  4. Add custom alerts for business metrics
  5. Schedule maintenance windows for deployments

âœ… Alerting Configuration Complete!
```

## Chains With

```bash
# Configure monitoring â†’ set alerts
/monitoring-configure && /alert-configure

# Configure alerts â†’ test with health check
/alert-configure && /agent-health-check

# Full observability setup
/monitoring-configure && /alert-configure && /log-stream

# Update alert thresholds after load test
/load-test && /alert-configure --threshold-latency 500
```

## See Also
- `/monitoring-configure` - Setup monitoring infrastructure
- `/agent-health-check` - Monitor agent health
- `/log-stream` - Real-time log streaming
- `/trace-request` - Distributed tracing
- `/profiler-start` - Performance profiling
